

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import KFold, train_test_split, cross_validate
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    accuracy_score, f1_score, roc_auc_score, confusion_matrix,
    RocCurveDisplay
)

import joblib

# -----------------------------
# 0) Load merged_df EXACTLY like Phase 2
# -----------------------------
if "merged_df" in globals() and isinstance(merged_df, pd.DataFrame):
    df = merged_df.copy()
    print("[OK] Using `merged_df` already in memory (from Phase 2).")
else:
    # Phase 2 saves: merged_df.to_csv('final_dataset_2019.csv', index=False)
    df = pd.read_csv("final_dataset_2019.csv")
    print("[OK] Loaded `final_dataset_2019.csv` (saved in Phase 2).")

# -----------------------------
# 1) Validate required columns (Phase 2 names)
# -----------------------------
REQUIRED = ["Happiness_Rank", "Index Score"]
for col in REQUIRED:
    if col not in df.columns:
        raise ValueError(f"Missing required column: '{col}'. Check your Phase 2 output/merge.")

# -----------------------------
# 2) Build FEATURES (Freedom-only, matched to Phase 2)
#    Use known freedom columns if they exist; fallback to all numeric except target.
# -----------------------------
econ_candidates = [
    "Index Score",
    "Property Rights",
    "Government Integrity",
    "Business Freedom",
    # optional freedom subscores (if present in your merged_df)
    "Judicial Effectiveness", "Fiscal Health", "Tax Burden", "Government Spending",
    "Monetary Freedom", "Trade Freedom", "Investment Freedom", "Financial Freedom",
    "Labor Freedom"
]

econ_features = [c for c in econ_candidates if c in df.columns]

# Fallback if too few are present
if len(econ_features) < 2:
    print("[WARN] Fewer than 2 freedom features found from the expected list.")
    print("[WARN] Falling back to all numeric columns (excluding the target).")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    econ_features = [c for c in numeric_cols if c != "Happiness_Rank"]

# Prepare clean dataset for ML
ml_df = df[["Happiness_Rank"] + econ_features].copy()
ml_df = ml_df.dropna(subset=["Happiness_Rank"])  # need target

X = ml_df[econ_features].copy()
y = ml_df["Happiness_Rank"].astype(float).copy()

print(f"[OK] Target: Happiness_Rank")
print(f"[OK] Features used ({len(econ_features)}): {econ_features}")

# -----------------------------
# 3) Train/Test split + CV
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

cv = KFold(n_splits=5, shuffle=True, random_state=42)

# -----------------------------
# 4) REGRESSION MODELS (Predict Happiness_Rank)
# -----------------------------
reg_models = {
    "LinearRegression": LinearRegression(),
    "Ridge": Ridge(alpha=1.0, random_state=42),
    "Lasso": Lasso(alpha=0.01, random_state=42),
    "ElasticNet": ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42),
    "RandomForest": RandomForestRegressor(
        n_estimators=500, random_state=42, min_samples_leaf=2
    ),
    "GradientBoosting": GradientBoostingRegressor(random_state=42),
}

linear_names = {"LinearRegression", "Ridge", "Lasso", "ElasticNet"}

def make_reg_pipeline(name, model):
    if name in linear_names:
        return Pipeline([
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
            ("model", model),
        ])
    else:
        return Pipeline([
            ("imputer", SimpleImputer(strategy="median")),
            ("model", model),
        ])

scoring_reg = {
    "MAE": "neg_mean_absolute_error",
    "RMSE": "neg_root_mean_squared_error",
    "R2": "r2",
}

reg_rows = []
best_reg_name, best_reg_pipe, best_reg_rmse = None, None, np.inf

for name, model in reg_models.items():
    pipe = make_reg_pipeline(name, model)

    scores = cross_validate(pipe, X, y, cv=cv, scoring=scoring_reg, n_jobs=-1)
    mae  = -scores["test_MAE"].mean()
    rmse = -scores["test_RMSE"].mean()
    r2   =  scores["test_R2"].mean()

    reg_rows.append([name, mae, rmse, r2])

    if rmse < best_reg_rmse:
        best_reg_rmse = rmse
        best_reg_name = name
        best_reg_pipe = pipe

reg_results = pd.DataFrame(reg_rows, columns=["Model", "CV_MAE", "CV_RMSE", "CV_R2"]).sort_values("CV_RMSE")
print("\n=== REGRESSION (5-Fold CV) ===")
print(reg_results.to_string(index=False))

# Fit best reg model and evaluate on test set
best_reg_pipe.fit(X_train, y_train)
pred = best_reg_pipe.predict(X_test)

test_mae = mean_absolute_error(y_test, pred)
test_rmse = mean_squared_error(y_test, pred, squared=False)
test_r2 = r2_score(y_test, pred)

print(f"\n[BEST REG MODEL] {best_reg_name}")
print(f"Test MAE : {test_mae:.4f}")
print(f"Test RMSE: {test_rmse:.4f}")
print(f"Test R^2 : {test_r2:.4f}")

# Save
reg_results.to_csv("ml_regression_cv_results.csv", index=False)
joblib.dump(best_reg_pipe, "best_regression_model.joblib")
print("[OK] Saved: ml_regression_cv_results.csv, best_regression_model.joblib")

# Plot actual vs predicted (NOTE: lower Happiness_Rank is "better" per your Phase 2 interpretation)
plt.figure(figsize=(7, 5))
plt.scatter(y_test, pred, alpha=0.75)
plt.title(f"Actual vs Predicted Happiness_Rank ({best_reg_name})")
plt.xlabel("Actual Happiness_Rank")
plt.ylabel("Predicted Happiness_Rank")
plt.grid(True, alpha=0.3)
plt.show()

# -----------------------------
# 5) Feature Importance (Permutation) for best reg model
# -----------------------------
perm = permutation_importance(
    best_reg_pipe, X_test, y_test,
    n_repeats=30, random_state=42, n_jobs=-1
)

imp_df = pd.DataFrame({
    "Feature": econ_features,
    "Importance": perm.importances_mean
}).sort_values("Importance", ascending=False)

imp_df.to_csv("ml_feature_importance.csv", index=False)
print("[OK] Saved: ml_feature_importance.csv")

TOPK = min(15, len(imp_df))
plt.figure(figsize=(8, 5))
plt.barh(imp_df["Feature"].head(TOPK)[::-1], imp_df["Importance"].head(TOPK)[::-1])
plt.title(f"Top {TOPK} Permutation Importances ({best_reg_name})")
plt.xlabel("Mean importance (drop in score when shuffled)")
plt.tight_layout()
plt.show()

# -----------------------------
# 6) CLASSIFICATION (High vs Low Happiness based on median rank)
#     Phase 2 meaning: lower rank = happier
# -----------------------------
# High happiness => Happiness_Rank <= median
median_rank = ml_df["Happiness_Rank"].median()
y_clf = (ml_df["Happiness_Rank"] <= median_rank).astype(int)

Xc = ml_df[econ_features].copy()
yc = y_clf.copy()

Xc_train, Xc_test, yc_train, yc_test = train_test_split(
    Xc, yc, test_size=0.2, random_state=42, stratify=yc
)

clf_models = {
    "LogisticRegression": LogisticRegression(max_iter=5000, random_state=42),
    "RandomForest": RandomForestClassifier(
        n_estimators=500, random_state=42, min_samples_leaf=2
    ),
    "GradientBoosting": GradientBoostingClassifier(random_state=42),
}

def make_clf_pipeline(name, model):
    if name == "LogisticRegression":
        return Pipeline([
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
            ("model", model),
        ])
    else:
        return Pipeline([
            ("imputer", SimpleImputer(strategy="median")),
            ("model", model),
        ])

scoring_clf = {"ACC": "accuracy", "F1": "f1", "AUC": "roc_auc"}

clf_rows = []
best_clf_name, best_clf_pipe, best_auc = None, None, -np.inf

for name, model in clf_models.items():
    pipe = make_clf_pipeline(name, model)
    scores = cross_validate(pipe, Xc, yc, cv=cv, scoring=scoring_clf, n_jobs=-1)

    acc = scores["test_ACC"].mean()
    f1  = scores["test_F1"].mean()
    auc = scores["test_AUC"].mean()

    clf_rows.append([name, acc, f1, auc])

    if auc > best_auc:
        best_auc = auc
        best_clf_name = name
        best_clf_pipe = pipe

clf_results = pd.DataFrame(clf_rows, columns=["Model", "CV_ACC", "CV_F1", "CV_AUC"]).sort_values("CV_AUC", ascending=False)
print("\n=== CLASSIFICATION (High vs Low Happiness; 5-Fold CV) ===")
print(clf_results.to_string(index=False))

# Fit best clf model and evaluate on test set
best_clf_pipe.fit(Xc_train, yc_train)
yc_pred = best_clf_pipe.predict(Xc_test)

if hasattr(best_clf_pipe.named_steps["model"], "predict_proba"):
    yc_prob = best_clf_pipe.predict_proba(Xc_test)[:, 1]
else:
    yc_prob = best_clf_pipe.decision_function(Xc_test)

acc = accuracy_score(yc_test, yc_pred)
f1 = f1_score(yc_test, yc_pred)
auc = roc_auc_score(yc_test, yc_prob)

print(f"\n[BEST CLF MODEL] {best_clf_name}")
print(f"Test ACC: {acc:.4f}")
print(f"Test F1 : {f1:.4f}")
print(f"Test AUC: {auc:.4f}")
print("\nConfusion Matrix:\n", confusion_matrix(yc_test, yc_pred))

RocCurveDisplay.from_predictions(yc_test, yc_prob)
plt.title(f"ROC Curve ({best_clf_name})")
plt.show()

# Save
clf_results.to_csv("ml_classification_cv_results.csv", index=False)
joblib.dump(best_clf_pipe, "best_classification_model.joblib")
print("[OK] Saved: ml_classification_cv_results.csv, best_classification_model.joblib")

print("\nâœ… Phase 3 ML finished (matched to Phase 2 column names).")

